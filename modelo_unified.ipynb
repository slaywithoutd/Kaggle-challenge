{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8c108d",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o de Modelos Preditivos para Sucesso de Startups\n",
    "\n",
    "## üéØ **Objetivo Principal**\n",
    "Superar a barreira de **80% de acur√°cia** na predi√ß√£o de sucesso de startups, mantendo a interpretabilidade dos resultados. \n",
    "\n",
    "**Meta:** Atingir ou superar acur√°cia ‚â• 80% atrav√©s de m√∫ltiplas estrat√©gias de otimiza√ß√£o.\n",
    "\n",
    "## üìä **Dataset Unificado**\n",
    "Utilizamos o dataset unificado que combina as melhores features das tr√™s hip√≥teses:\n",
    "- **H1 - Capital**: Features de financiamento e crescimento\n",
    "- **H2 - Geografia**: Features de localiza√ß√£o\n",
    "- **H3 - Operacional**: Features de maturidade operacional\n",
    "\n",
    "## üöÄ **Estrat√©gias de Otimiza√ß√£o**\n",
    "1. **Teste de M√∫ltiplos Algoritmos**: Logistic Regression, Random Forest, Gradient Boosting, SVM, XGBoost\n",
    "2. **Ajuste de Hiperpar√¢metros**: GridSearchCV e RandomizedSearchCV\n",
    "3. **Engenharia de Features**: Sele√ß√£o, intera√ß√µes e transforma√ß√µes\n",
    "4. **T√©cnicas de Ensemble**: Voting, Stacking e Bagging\n",
    "5. **Balanceamento de Classes**: SMOTE, class_weight, threshold tuning\n",
    "\n",
    "## üìà **M√©tricas de Avalia√ß√£o**\n",
    "- Acur√°cia (objetivo principal ‚â• 80%)\n",
    "- Precis√£o, Recall, F1-Score\n",
    "- ROC-AUC e Precision-Recall AUC\n",
    "- Matriz de Confus√£o detalhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas para otimiza√ß√£o de modelos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelos de Machine Learning\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, BaggingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# M√©tricas e avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, make_scorer\n",
    ")\n",
    "\n",
    "# Pr√©-processamento e feature engineering\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Balanceamento de classes\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_available = True\n",
    "    print(\"‚úÖ XGBoost dispon√≠vel\")\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"‚ö†Ô∏è XGBoost n√£o dispon√≠vel - usando apenas RF e GB\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas de otimiza√ß√£o importadas\")\n",
    "print(\"üöÄ Iniciando processo de otimiza√ß√£o para atingir 80% de acur√°cia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb43a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset unificado\n",
    "print(\"üìä CARREGANDO DATASET UNIFICADO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_df = pd.read_csv('database/unified_train.csv')\n",
    "test_df = pd.read_csv('database/unified_test.csv')\n",
    "features_summary = pd.read_csv('database/unified_features_summary.csv')\n",
    "\n",
    "print(f\"‚úÖ Dados carregados:\")\n",
    "print(f\"   ‚Ä¢ Train: {train_df.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {test_df.shape}\")\n",
    "print(f\"   ‚Ä¢ Features: {len(features_summary)}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = train_df.drop('labels', axis=1)\n",
    "y = train_df['labels']\n",
    "X_test = test_df.copy()\n",
    "\n",
    "print(f\"\\nüìã Estrutura dos dados:\")\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Test set: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Distribui√ß√£o target: Sucesso {y.mean():.1%}, Fracasso {1-y.mean():.1%}\")\n",
    "\n",
    "# Verificar consist√™ncia\n",
    "print(f\"\\nüîç Verifica√ß√µes de qualidade:\")\n",
    "print(f\"   ‚Ä¢ Valores nulos em X: {X.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Valores nulos em X_test: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Features consistentes: {set(X.columns) == set(X_test.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fca2ae",
   "metadata": {},
   "source": [
    "## üìä **An√°lise Inicial dos Dados para Otimiza√ß√£o**\n",
    "\n",
    "Antes de implementar as estrat√©gias de otimiza√ß√£o, vamos analisar os dados para identificar desafios espec√≠ficos que podem estar limitando a acur√°cia atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3124618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise inicial para identificar desafios na otimiza√ß√£o\n",
    "print(\"üîç AN√ÅLISE INICIAL PARA OTIMIZA√á√ÉO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# An√°lise da distribui√ß√£o de classes\n",
    "class_distribution = y.value_counts(normalize=True)\n",
    "print(f\"\\nüìä Distribui√ß√£o de classes:\")\n",
    "print(f\"   ‚Ä¢ Classe 0 (Fracasso): {class_distribution[0]:.3f} ({class_distribution[0]*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Classe 1 (Sucesso): {class_distribution[1]:.3f} ({class_distribution[1]*100:.1f}%)\")\n",
    "\n",
    "imbalance_ratio = class_distribution[0] / class_distribution[1]\n",
    "print(f\"   ‚Ä¢ Ratio de desbalanceamento: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"   ‚ö†Ô∏è DESBALANCEAMENTO DETECTADO - Implementar t√©cnicas de balanceamento\")\n",
    "\n",
    "# An√°lise de features correlacionadas\n",
    "print(f\"\\nüîó An√°lise de correla√ß√µes:\")\n",
    "corr_matrix = X.corr()\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = abs(corr_matrix.iloc[i, j])\n",
    "        if corr_val > 0.8:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"   ‚ö†Ô∏è {len(high_corr_pairs)} pares de features altamente correlacionadas (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs[:3]:\n",
    "        print(f\"      ‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Nenhuma correla√ß√£o excessiva detectada\")\n",
    "\n",
    "# An√°lise de features com baixa vari√¢ncia\n",
    "low_variance_features = []\n",
    "for col in X.columns:\n",
    "    if X[col].std() < 0.01:\n",
    "        low_variance_features.append(col)\n",
    "\n",
    "if low_variance_features:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Features com baixa vari√¢ncia: {low_variance_features}\")\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ Todas as features apresentam vari√¢ncia adequada\")\n",
    "\n",
    "print(f\"\\nüéØ ESTRAT√âGIAS RECOMENDADAS:\")\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"   1. Aplicar t√©cnicas de balanceamento (class_weight, SMOTE)\")\n",
    "if high_corr_pairs:\n",
    "    print(\"   2. Considerar remo√ß√£o de features correlacionadas\")\n",
    "print(\"   3. Testar ensemble methods para melhor performance\")\n",
    "print(\"   4. Ajustar hiperpar√¢metros com valida√ß√£o cruzada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012fc63",
   "metadata": {},
   "source": [
    "## üîß **Estrat√©gia 1: Otimiza√ß√£o de Hiperpar√¢metros com GridSearchCV**\n",
    "\n",
    "Implementa√ß√£o sistem√°tica de busca em grade para encontrar os melhores hiperpar√¢metros para cada algoritmo. Esta estrat√©gia foca na maximiza√ß√£o da acur√°cia atrav√©s do ajuste fino dos par√¢metros.\n",
    "\n",
    "**Objetivos:**\n",
    "- Otimizar Random Forest, Gradient Boosting e XGBoost\n",
    "- Usar valida√ß√£o cruzada estratificada (5-fold)\n",
    "- Comparar performance antes e depois da otimiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRAT√âGIA 1: OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS\n",
    "print(\"üîß ESTRAT√âGIA 1: OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fun√ß√£o para avalia√ß√£o padronizada\n",
    "def avaliar_modelo(model, X_test, y_test, nome_modelo):\n",
    "    \"\"\"Fun√ß√£o padronizada para avalia√ß√£o de modelos\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nüéØ {nome_modelo}:\")\n",
    "    print(f\"   ‚Ä¢ Acur√°cia: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Precis√£o: {precision:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {recall:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'modelo': nome_modelo,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Configura√ß√£o dos par√¢metros para otimiza√ß√£o\n",
    "param_grids = {}\n",
    "\n",
    "# Random Forest sempre dispon√≠vel\n",
    "param_grids['RandomForest'] = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Gradient Boosting sempre dispon√≠vel\n",
    "param_grids['GradientBoosting'] = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# XGBoost apenas se dispon√≠vel\n",
    "if xgb_available:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "# Divis√£o dos dados para treinamento e valida√ß√£o\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Divis√£o dos dados:\")\n",
    "print(f\"   ‚Ä¢ Train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {X_test.shape}\")\n",
    "\n",
    "# Implementa√ß√£o do GridSearchCV para cada modelo\n",
    "modelos_otimizados = {}\n",
    "resultados_otimizacao = []\n",
    "\n",
    "print(\"\\nüîç Iniciando otimiza√ß√£o de hiperpar√¢metros...\")\n",
    "print(\"‚è±Ô∏è Este processo pode demorar alguns minutos...\")\n",
    "\n",
    "for nome_modelo, params in param_grids.items():\n",
    "    print(f\"\\nüîÑ Otimizando {nome_modelo}...\")\n",
    "    \n",
    "    # Definir o modelo base\n",
    "    if nome_modelo == 'RandomForest':\n",
    "        modelo_base = RandomForestClassifier(random_state=42)\n",
    "    elif nome_modelo == 'GradientBoosting':\n",
    "        modelo_base = GradientBoostingClassifier(random_state=42)\n",
    "    elif nome_modelo == 'XGBoost' and xgb_available:\n",
    "        modelo_base = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # GridSearchCV com valida√ß√£o cruzada estratificada\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=modelo_base,\n",
    "        param_grid=params,\n",
    "        scoring='accuracy',\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar e encontrar melhores par√¢metros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Armazenar modelo otimizado\n",
    "    modelos_otimizados[nome_modelo] = grid_search.best_estimator_\n",
    "    \n",
    "    # Avaliar modelo otimizado\n",
    "    resultado = avaliar_modelo(grid_search.best_estimator_, X_test, y_test, f\"{nome_modelo} (Otimizado)\")\n",
    "    resultado['best_params'] = grid_search.best_params_\n",
    "    resultado['cv_score'] = grid_search.best_score_\n",
    "    resultados_otimizacao.append(resultado)\n",
    "    \n",
    "    print(f\"   üìà Melhor CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"   üéõÔ∏è Melhores par√¢metros: {grid_search.best_params_}\")\n",
    "\n",
    "print(\"\\n‚úÖ Otimiza√ß√£o de hiperpar√¢metros conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bb6a3",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è **Estrat√©gia 2: Balanceamento de Classes e Feature Engineering**\n",
    "\n",
    "Para lidar com poss√≠vel desbalanceamento de classes e melhorar a qualidade das features. Esta estrat√©gia implementa t√©cnicas de preprocessamento avan√ßadas para maximizar o potencial dos dados.\n",
    "\n",
    "**T√©cnicas implementadas:**\n",
    "- **Class Weight Balancing**: Ajuste autom√°tico de pesos das classes\n",
    "- **SMOTE**: Synthetic Minority Oversampling Technique\n",
    "- **Feature Scaling**: Normaliza√ß√£o e padroniza√ß√£o\n",
    "- **Feature Selection**: Sele√ß√£o das features mais importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRAT√âGIA 2: BALANCEAMENTO E FEATURE ENGINEERING\n",
    "print(\"‚öñÔ∏è ESTRAT√âGIA 2: BALANCEAMENTO E FEATURE ENGINEERING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Importar bibliotecas espec√≠ficas para balanceamento\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    smote_available = True\n",
    "    print(\"‚úÖ SMOTE dispon√≠vel\")\n",
    "except ImportError:\n",
    "    smote_available = False\n",
    "    print(\"‚ö†Ô∏è SMOTE n√£o dispon√≠vel - usando apenas class_weight\")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 1. AN√ÅLISE E SELE√á√ÉO DE FEATURES\n",
    "print(\"\\nüîç SELE√á√ÉO DE FEATURES:\")\n",
    "\n",
    "# Sele√ß√£o univariada com SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=15)  # Seleciona top 15 features\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Obter nomes das features selecionadas\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"üìä Top 15 features selecionadas:\")\n",
    "for feat, score in zip(selected_features, feature_scores):\n",
    "    print(f\"   ‚Ä¢ {feat}: {score:.2f}\")\n",
    "\n",
    "# 2. FEATURE SCALING\n",
    "print(f\"\\nüéöÔ∏è NORMALIZA√á√ÉO DE FEATURES:\")\n",
    "\n",
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "print(\"   ‚úÖ Features normalizadas com StandardScaler\")\n",
    "\n",
    "# 3. T√âCNICAS DE BALANCEAMENTO\n",
    "print(f\"\\n‚öñÔ∏è BALANCEAMENTO DE CLASSES:\")\n",
    "\n",
    "# Classe com class_weight\n",
    "modelos_balanceados = {}\n",
    "\n",
    "# Random Forest com class_weight balanced\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "rf_balanced.fit(X_train_scaled, y_train)\n",
    "modelos_balanceados['RF_ClassWeight'] = rf_balanced\n",
    "\n",
    "print(\"   ‚úÖ Random Forest com class_weight='balanced'\")\n",
    "\n",
    "# Gradient Boosting (n√£o tem class_weight nativo, usamos sample_weight)\n",
    "gb_balanced = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calcular sample_weight manualmente para balanceamento\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "gb_balanced.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "modelos_balanceados['GB_SampleWeight'] = gb_balanced\n",
    "\n",
    "print(\"   ‚úÖ Gradient Boosting com sample_weight balanceado\")\n",
    "\n",
    "# 4. SMOTE (se dispon√≠vel)\n",
    "if smote_available:\n",
    "    print(\"\\nüîÑ Aplicando SMOTE:\")\n",
    "    \n",
    "    # SMOTE para oversampling da classe minorit√°ria\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"   üìà Dataset original: {len(y_train)} amostras\")\n",
    "    print(f\"   üìà Dataset com SMOTE: {len(y_train_smote)} amostras\")\n",
    "    print(f\"   üìä Nova distribui√ß√£o: {np.bincount(y_train_smote)}\")\n",
    "    \n",
    "    # Treinar modelos com dados balanceados por SMOTE\n",
    "    rf_smote = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)\n",
    "    rf_smote.fit(X_train_smote, y_train_smote)\n",
    "    modelos_balanceados['RF_SMOTE'] = rf_smote\n",
    "    \n",
    "    gb_smote = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    gb_smote.fit(X_train_smote, y_train_smote)\n",
    "    modelos_balanceados['GB_SMOTE'] = gb_smote\n",
    "    \n",
    "    print(\"   ‚úÖ Modelos treinados com SMOTE\")\n",
    "\n",
    "# 5. AVALIA√á√ÉO DOS MODELOS BALANCEADOS\n",
    "print(f\"\\nüìä AVALIA√á√ÉO DOS MODELOS BALANCEADOS:\")\n",
    "\n",
    "resultados_balanceamento = []\n",
    "\n",
    "for nome_modelo, modelo in modelos_balanceados.items():\n",
    "    resultado = avaliar_modelo(modelo, X_test_scaled, y_test, nome_modelo)\n",
    "    resultados_balanceamento.append(resultado)\n",
    "\n",
    "print(\"\\n‚úÖ Estrat√©gia de balanceamento conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e0a27",
   "metadata": {},
   "source": [
    "## üé≠ **Estrat√©gia 3: Ensemble Methods e Stacking**\n",
    "\n",
    "Combina√ß√£o inteligente de m√∫ltiplos modelos para maximizar a performance. Esta estrat√©gia utiliza a sabedoria coletiva de diferentes algoritmos para superar limita√ß√µes individuais.\n",
    "\n",
    "**M√©todos de Ensemble:**\n",
    "- **Voting Classifier**: Combina√ß√£o por vota√ß√£o (hard e soft voting)\n",
    "- **Bagging**: Bootstrap Aggregating com diferentes algoritmos\n",
    "- **Stacking**: Meta-learner para combinar predi√ß√µes de base learners\n",
    "- **Blending**: M√©dia ponderada das predi√ß√µes dos melhores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66910ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRAT√âGIA 3: ENSEMBLE METHODS E STACKING\n",
    "print(\"üé≠ ESTRAT√âGIA 3: ENSEMBLE METHODS E STACKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Preparar modelos base (usar os melhores modelos das estrat√©gias anteriores)\n",
    "print(\"üîß Preparando modelos base para ensemble...\")\n",
    "\n",
    "# Modelos base otimizados\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=15, min_samples_split=2, \n",
    "        class_weight='balanced', random_state=42\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=5, \n",
    "        min_samples_split=2, random_state=42\n",
    "    )),\n",
    "    ('svm', SVC(\n",
    "        C=1.0, kernel='rbf', probability=True, \n",
    "        class_weight='balanced', random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Adicionar XGBoost apenas se dispon√≠vel\n",
    "if xgb_available:\n",
    "    base_models.append(('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=5, \n",
    "        random_state=42, eval_metric='logloss'\n",
    "    )))\n",
    "\n",
    "print(f\"   ‚úÖ {len(base_models)} modelos base preparados\")\n",
    "\n",
    "# 1. VOTING CLASSIFIER\n",
    "print(f\"\\nüó≥Ô∏è VOTING CLASSIFIER:\")\n",
    "\n",
    "# Hard Voting\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='hard'\n",
    ")\n",
    "voting_hard.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Soft Voting\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='soft'\n",
    ")\n",
    "voting_soft.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"   ‚úÖ Hard Voting e Soft Voting treinados\")\n",
    "\n",
    "# 2. BAGGING ENSEMBLE\n",
    "print(f\"\\nüéí BAGGING ENSEMBLE:\")\n",
    "\n",
    "# Bagging com diferentes modelos base\n",
    "bagging_rf = BaggingClassifier(\n",
    "    estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "bagging_gb = BaggingClassifier(\n",
    "    estimator=GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"   ‚úÖ Bagging com RF e GB treinados\")\n",
    "\n",
    "# 3. STACKING CLASSIFIER\n",
    "print(f\"\\nüèóÔ∏è STACKING CLASSIFIER:\")\n",
    "\n",
    "# Meta-learner (Logistic Regression)\n",
    "meta_learner = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Stacking com valida√ß√£o cruzada\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "stacking_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"   ‚úÖ Stacking Classifier treinado\")\n",
    "\n",
    "# 4. BLENDING PERSONALIZADO\n",
    "print(f\"\\nüçØ BLENDING PERSONALIZADO:\")\n",
    "\n",
    "# Treinar modelos individuais e obter predi√ß√µes\n",
    "individual_predictions = {}\n",
    "individual_probabilities = {}\n",
    "\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    individual_predictions[name] = model.predict(X_test_scaled)\n",
    "    individual_probabilities[name] = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Criar fun√ß√£o de blending com pesos otimizados\n",
    "def create_blended_prediction(probabilities_dict, weights=None):\n",
    "    \"\"\"Criar predi√ß√£o blended com pesos personalizados\"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.array([0.25, 0.25, 0.3, 0.2])  # Pesos balanceados\n",
    "    \n",
    "    blended_proba = np.zeros(len(list(probabilities_dict.values())[0]))\n",
    "    \n",
    "    for i, (name, proba) in enumerate(probabilities_dict.items()):\n",
    "        blended_proba += weights[i] * proba\n",
    "    \n",
    "    return (blended_proba > 0.5).astype(int), blended_proba\n",
    "\n",
    "# Aplicar blending\n",
    "blended_pred, blended_proba = create_blended_prediction(individual_probabilities)\n",
    "\n",
    "print(\"   ‚úÖ Blending personalizado aplicado\")\n",
    "\n",
    "# 5. AVALIA√á√ÉO DOS ENSEMBLE METHODS\n",
    "print(f\"\\nüìä AVALIA√á√ÉO DOS ENSEMBLE METHODS:\")\n",
    "\n",
    "ensemble_models = {\n",
    "    'Voting_Hard': voting_hard,\n",
    "    'Voting_Soft': voting_soft,\n",
    "    'Bagging_RF': bagging_rf,\n",
    "    'Bagging_GB': bagging_gb,\n",
    "    'Stacking': stacking_clf\n",
    "}\n",
    "\n",
    "resultados_ensemble = []\n",
    "\n",
    "# Avaliar modelos de ensemble tradicionais\n",
    "for nome_modelo, modelo in ensemble_models.items():\n",
    "    resultado = avaliar_modelo(modelo, X_test_scaled, y_test, nome_modelo)\n",
    "    resultados_ensemble.append(resultado)\n",
    "\n",
    "# Avaliar blending personalizado\n",
    "blended_accuracy = accuracy_score(y_test, blended_pred)\n",
    "blended_precision = precision_score(y_test, blended_pred)\n",
    "blended_recall = recall_score(y_test, blended_pred)\n",
    "blended_f1 = f1_score(y_test, blended_pred)\n",
    "\n",
    "print(f\"\\nüéØ Blending_Personalizado:\")\n",
    "print(f\"   ‚Ä¢ Acur√°cia: {blended_accuracy:.4f} ({blended_accuracy*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Precis√£o: {blended_precision:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {blended_recall:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {blended_f1:.4f}\")\n",
    "\n",
    "resultados_ensemble.append({\n",
    "    'modelo': 'Blending_Personalizado',\n",
    "    'accuracy': blended_accuracy,\n",
    "    'precision': blended_precision,\n",
    "    'recall': blended_recall,\n",
    "    'f1': blended_f1,\n",
    "    'predictions': blended_pred,\n",
    "    'probabilities': blended_proba\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ Ensemble methods conclu√≠dos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f488bc7",
   "metadata": {},
   "source": [
    "## üìà **Compara√ß√£o Final e Sele√ß√£o do Melhor Modelo**\n",
    "\n",
    "An√°lise comparativa completa de todas as estrat√©gias implementadas. Esta se√ß√£o consolida os resultados e identifica o modelo com melhor performance para atingir nossa meta de **‚â•80% de acur√°cia**.\n",
    "\n",
    "**Crit√©rios de avalia√ß√£o:**\n",
    "- **Acur√°cia prim√°ria**: Meta de ‚â•80%\n",
    "- **Estabilidade**: Consist√™ncia entre m√©tricas\n",
    "- **Interpretabilidade**: Capacidade de explica√ß√£o\n",
    "- **Generaliza√ß√£o**: Performance em valida√ß√£o cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARA√á√ÉO FINAL E SELE√á√ÉO DO MELHOR MODELO\n",
    "print(\"üìà COMPARA√á√ÉO FINAL E SELE√á√ÉO DO MELHOR MODELO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Consolidar todos os resultados\n",
    "todos_resultados = []\n",
    "\n",
    "# Adicionar resultados das tr√™s estrat√©gias\n",
    "if 'resultados_otimizacao' in locals():\n",
    "    todos_resultados.extend(resultados_otimizacao)\n",
    "    \n",
    "if 'resultados_balanceamento' in locals():\n",
    "    todos_resultados.extend(resultados_balanceamento)\n",
    "    \n",
    "if 'resultados_ensemble' in locals():\n",
    "    todos_resultados.extend(resultados_ensemble)\n",
    "\n",
    "# Criar DataFrame para compara√ß√£o\n",
    "df_comparacao = pd.DataFrame(todos_resultados)\n",
    "\n",
    "# Ordenar por acur√°cia decrescente\n",
    "df_comparacao = df_comparacao.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"üèÜ RANKING DOS MODELOS (por acur√°cia):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, row in df_comparacao.iterrows():\n",
    "    accuracy_pct = row['accuracy'] * 100\n",
    "    status = \"üéØ META ATINGIDA!\" if row['accuracy'] >= 0.80 else \"‚ùå Abaixo da meta\"\n",
    "    \n",
    "    print(f\"{idx+1:2d}. {row['modelo']:<25} | \"\n",
    "          f\"Acc: {accuracy_pct:5.2f}% | \"\n",
    "          f\"Prec: {row['precision']:.3f} | \"\n",
    "          f\"Rec: {row['recall']:.3f} | \"\n",
    "          f\"F1: {row['f1']:.3f} | \"\n",
    "          f\"{status}\")\n",
    "\n",
    "# Identificar modelos que atingiram a meta\n",
    "modelos_meta = df_comparacao[df_comparacao['accuracy'] >= 0.80]\n",
    "\n",
    "print(f\"\\nüéØ MODELOS QUE ATINGIRAM A META (‚â•80%):\")\n",
    "if len(modelos_meta) > 0:\n",
    "    print(f\"   ‚úÖ {len(modelos_meta)} modelo(s) atingiu(ram) a meta!\")\n",
    "    \n",
    "    # Selecionar o melhor modelo\n",
    "    melhor_modelo = modelos_meta.iloc[0]\n",
    "    print(f\"\\nü•á MELHOR MODELO SELECIONADO:\")\n",
    "    print(f\"   üìõ Nome: {melhor_modelo['modelo']}\")\n",
    "    print(f\"   üéØ Acur√°cia: {melhor_modelo['accuracy']:.4f} ({melhor_modelo['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   üéóÔ∏è Precis√£o: {melhor_modelo['precision']:.4f}\")\n",
    "    print(f\"   üîÑ Recall: {melhor_modelo['recall']:.4f}\")\n",
    "    print(f\"   ‚öñÔ∏è F1-Score: {melhor_modelo['f1']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"   ‚ùå Nenhum modelo atingiu a meta de 80%\")\n",
    "    melhor_modelo = df_comparacao.iloc[0]\n",
    "    print(f\"\\nü•à MELHOR MODELO DISPON√çVEL:\")\n",
    "    print(f\"   üìõ Nome: {melhor_modelo['modelo']}\")\n",
    "    print(f\"   üéØ Acur√°cia: {melhor_modelo['accuracy']:.4f} ({melhor_modelo['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   üí° Recomenda√ß√£o: Considerar mais dados ou features adicionais\")\n",
    "\n",
    "# Visualizar compara√ß√£o\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Gr√°fico de barras com acur√°cia\n",
    "plt.subplot(2, 2, 1)\n",
    "colors = ['green' if acc >= 0.80 else 'orange' if acc >= 0.75 else 'red' \n",
    "          for acc in df_comparacao['accuracy']]\n",
    "\n",
    "plt.barh(range(len(df_comparacao)), df_comparacao['accuracy']*100, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(df_comparacao)), df_comparacao['modelo'], fontsize=8)\n",
    "plt.xlabel('Acur√°cia (%)')\n",
    "plt.title('Compara√ß√£o de Acur√°cia dos Modelos')\n",
    "plt.axvline(x=80, color='red', linestyle='--', alpha=0.7, label='Meta (80%)')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Scatter plot: Precision vs Recall\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_comparacao['precision'], df_comparacao['recall'], \n",
    "           c=[colors[i] for i in range(len(df_comparacao))], alpha=0.7, s=100)\n",
    "plt.xlabel('Precis√£o')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precis√£o vs Recall')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compara√ß√£o de m√©tricas (radar/spider)\n",
    "plt.subplot(2, 2, 3)\n",
    "metricas = ['accuracy', 'precision', 'recall', 'f1']\n",
    "top3_models = df_comparacao.head(3)\n",
    "\n",
    "x = np.arange(len(metricas))\n",
    "width = 0.25\n",
    "\n",
    "for i, (idx, model) in enumerate(top3_models.iterrows()):\n",
    "    valores = [model[m] for m in metricas]\n",
    "    plt.bar(x + i*width, valores, width, label=model['modelo'][:15], alpha=0.7)\n",
    "\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Top 3 Modelos - Todas as M√©tricas')\n",
    "plt.xticks(x + width, metricas)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score ranking\n",
    "plt.subplot(2, 2, 4)\n",
    "df_f1_sorted = df_comparacao.sort_values('f1', ascending=True)\n",
    "plt.barh(range(len(df_f1_sorted)), df_f1_sorted['f1'], \n",
    "         color='purple', alpha=0.7)\n",
    "plt.yticks(range(len(df_f1_sorted)), df_f1_sorted['modelo'], fontsize=8)\n",
    "plt.xlabel('F1-Score')\n",
    "plt.title('Ranking por F1-Score')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matriz de confus√£o do melhor modelo\n",
    "if 'predictions' in melhor_modelo and melhor_modelo['predictions'] is not None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    plt.subplot(1, 2, 1)\n",
    "    cm = confusion_matrix(y_test, melhor_modelo['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fracasso', 'Sucesso'], \n",
    "                yticklabels=['Fracasso', 'Sucesso'])\n",
    "    plt.title(f'Matriz de Confus√£o - {melhor_modelo[\"modelo\"]}')\n",
    "    plt.ylabel('Valores Reais')\n",
    "    plt.xlabel('Predi√ß√µes')\n",
    "    \n",
    "    # Curva ROC (se houver probabilidades)\n",
    "    if 'probabilities' in melhor_modelo and melhor_modelo['probabilities'] is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        fpr, tpr, _ = roc_curve(y_test, melhor_modelo['probabilities'])\n",
    "        auc_score = roc_auc_score(y_test, melhor_modelo['probabilities'])\n",
    "        \n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc_score:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        plt.xlabel('Taxa de Falsos Positivos')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "        plt.title(f'Curva ROC - {melhor_modelo[\"modelo\"]}')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lise comparativa conclu√≠da!\")\n",
    "print(f\"üéØ {'META ATINGIDA!' if melhor_modelo['accuracy'] >= 0.80 else 'Meta n√£o atingida - considerar otimiza√ß√µes adicionais'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80628ff",
   "metadata": {},
   "source": [
    "## üéØ **Gera√ß√£o de Predi√ß√µes Finais para Submiss√£o**\n",
    "\n",
    "Com o melhor modelo selecionado, geramos as predi√ß√µes finais para o conjunto de teste e preparamos o arquivo de submiss√£o no formato requerido pela competi√ß√£o.\n",
    "\n",
    "**Processo de submiss√£o:**\n",
    "1. Aplicar mesmo preprocessamento usado no treino\n",
    "2. Gerar predi√ß√µes com o melhor modelo\n",
    "3. Criar arquivo `submission.csv`\n",
    "4. Validar formato e exportar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217074e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERA√á√ÉO DE PREDI√á√ïES FINAIS PARA SUBMISS√ÉO\n",
    "print(\"üéØ GERA√á√ÉO DE PREDI√á√ïES FINAIS PARA SUBMISS√ÉO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Carregar dados de teste para submiss√£o\n",
    "print(\"üìÅ Carregando dados de teste...\")\n",
    "test_data = pd.read_csv('database/unified_test.csv')\n",
    "\n",
    "print(f\"   ‚úÖ Dados de teste carregados: {test_data.shape}\")\n",
    "print(f\"   üìä Features dispon√≠veis: {test_data.columns.tolist()}\")\n",
    "\n",
    "# Preparar dados de teste com mesmo preprocessamento\n",
    "print(f\"\\nüîß Aplicando preprocessamento...\")\n",
    "\n",
    "# Verificar se precisamos aplicar feature selection e scaling\n",
    "if 'selected_features' in locals() and 'scaler' in locals():\n",
    "    # Aplicar mesma sele√ß√£o de features usada no melhor modelo\n",
    "    test_data_selected = test_data[selected_features]\n",
    "    test_data_scaled = scaler.transform(test_data_selected)\n",
    "    print(f\"   ‚úÖ Feature selection aplicada: {len(selected_features)} features\")\n",
    "    print(f\"   ‚úÖ Normaliza√ß√£o aplicada\")\n",
    "    X_test_final = test_data_scaled\n",
    "else:\n",
    "    # Usar dados originais se n√£o houve preprocessing especial\n",
    "    X_test_final = test_data.values\n",
    "    print(f\"   ‚ÑπÔ∏è Usando dados originais (sem preprocessing especial)\")\n",
    "\n",
    "# Recuperar o melhor modelo para fazer predi√ß√µes\n",
    "if 'melhor_modelo' in locals():\n",
    "    modelo_final = melhor_modelo['modelo']\n",
    "    print(f\"\\nü•á Usando modelo: {modelo_final}\")\n",
    "    \n",
    "    # Determinar qual modelo usar baseado no nome\n",
    "    if 'Voting_Soft' in modelo_final and 'voting_soft' in locals():\n",
    "        modelo_predicao = voting_soft\n",
    "    elif 'Stacking' in modelo_final and 'stacking_clf' in locals():\n",
    "        modelo_predicao = stacking_clf\n",
    "    elif 'Blending' in modelo_final:\n",
    "        # Para blending, precisamos usar os modelos individuais\n",
    "        print(\"   üçØ Usando blending personalizado\")\n",
    "        modelo_predicao = None  # Ser√° tratado separadamente\n",
    "    elif 'RF_ClassWeight' in modelo_final and 'modelos_balanceados' in locals():\n",
    "        modelo_predicao = modelos_balanceados['RF_ClassWeight']\n",
    "    elif 'XGBoost' in modelo_final and 'modelos_otimizados' in locals() and xgb_available:\n",
    "        modelo_predicao = modelos_otimizados['XGBoost']\n",
    "    else:\n",
    "        # Fallback para um modelo padr√£o\n",
    "        print(\"   ‚ö†Ô∏è Modelo espec√≠fico n√£o encontrado, usando Random Forest otimizado\")\n",
    "        modelo_predicao = RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=15, class_weight='balanced', random_state=42\n",
    "        )\n",
    "        modelo_predicao.fit(X_train_scaled if 'X_train_scaled' in locals() else X_train, y_train)\n",
    "    \n",
    "    # Gerar predi√ß√µes\n",
    "    print(f\"\\nüîÆ Gerando predi√ß√µes...\")\n",
    "    \n",
    "    if modelo_predicao is not None:\n",
    "        # Predi√ß√µes com modelo padr√£o\n",
    "        predicoes_finais = modelo_predicao.predict(X_test_final)\n",
    "        probabilidades_finais = modelo_predicao.predict_proba(X_test_final)[:, 1] if hasattr(modelo_predicao, 'predict_proba') else None\n",
    "    else:\n",
    "        # Predi√ß√µes com blending (se aplic√°vel)\n",
    "        if 'individual_probabilities' in locals() and 'base_models' in locals():\n",
    "            print(\"   üçØ Aplicando blending...\")\n",
    "            test_probabilities = {}\n",
    "            \n",
    "            for name, model in base_models:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    test_probabilities[name] = model.predict_proba(X_test_final)[:, 1]\n",
    "            \n",
    "            predicoes_finais, probabilidades_finais = create_blended_prediction(test_probabilities)\n",
    "        else:\n",
    "            # Fallback final\n",
    "            print(\"   ‚ö†Ô∏è Usando modelo Random Forest como fallback\")\n",
    "            rf_fallback = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "            rf_fallback.fit(X_train, y_train)\n",
    "            predicoes_finais = rf_fallback.predict(X_test_final)\n",
    "            probabilidades_finais = rf_fallback.predict_proba(X_test_final)[:, 1]\n",
    "    \n",
    "    # Estat√≠sticas das predi√ß√µes\n",
    "    unique_preds, counts = np.unique(predicoes_finais, return_counts=True)\n",
    "    print(f\"\\nüìä Estat√≠sticas das predi√ß√µes:\")\n",
    "    for pred, count in zip(unique_preds, counts):\n",
    "        percentage = (count / len(predicoes_finais)) * 100\n",
    "        label = \"Fracasso\" if pred == 0 else \"Sucesso\"\n",
    "        print(f\"   ‚Ä¢ {label} (classe {pred}): {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Preparar arquivo de submiss√£o\n",
    "    print(f\"\\nüìù Preparando arquivo de submiss√£o...\")\n",
    "    \n",
    "    # Verificar se existe um template de submiss√£o\n",
    "    try:\n",
    "        sample_submission = pd.read_csv('database/sample_submission.csv')\n",
    "        print(f\"   ‚úÖ Template de submiss√£o encontrado: {sample_submission.shape}\")\n",
    "        \n",
    "        # Usar IDs do template de submiss√£o\n",
    "        submission = sample_submission.copy()\n",
    "        if len(submission) == len(predicoes_finais):\n",
    "            # Assumir que a coluna de target √© a segunda coluna\n",
    "            target_column = submission.columns[1]\n",
    "            submission[target_column] = predicoes_finais\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Incompatibilidade de tamanho: template={len(submission)}, predi√ß√µes={len(predicoes_finais)}\")\n",
    "            # Criar submiss√£o manual\n",
    "            submission = pd.DataFrame({\n",
    "                'id': range(len(predicoes_finais)),\n",
    "                'target': predicoes_finais\n",
    "            })\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ÑπÔ∏è Template n√£o encontrado, criando arquivo manual\")\n",
    "        submission = pd.DataFrame({\n",
    "            'id': range(len(predicoes_finais)),\n",
    "            'target': predicoes_finais\n",
    "        })\n",
    "    \n",
    "    # Salvar arquivo de submiss√£o\n",
    "    submission_path = 'database/submission_otimizado.csv'\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Arquivo salvo: {submission_path}\")\n",
    "    print(f\"   üìã Formato: {submission.shape}\")\n",
    "    print(f\"   üè∑Ô∏è Colunas: {submission.columns.tolist()}\")\n",
    "    \n",
    "    # Mostrar preview da submiss√£o\n",
    "    print(f\"\\nüëÄ Preview da submiss√£o:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    # Salvar tamb√©m probabilidades se dispon√≠veis\n",
    "    if probabilidades_finais is not None:\n",
    "        prob_submission = submission.copy()\n",
    "        prob_submission['probability'] = probabilidades_finais\n",
    "        prob_path = 'database/submission_com_probabilidades.csv'\n",
    "        prob_submission.to_csv(prob_path, index=False)\n",
    "        print(f\"   üíæ Arquivo com probabilidades salvo: {prob_path}\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUBMISS√ÉO PRONTA!\")\n",
    "    print(f\"   üìà Modelo usado: {modelo_final}\")\n",
    "    print(f\"   üéØ Acur√°cia estimada: {melhor_modelo['accuracy']:.4f} ({melhor_modelo['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   üìÅ Arquivo principal: {submission_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum modelo foi selecionado. Execute as c√©lulas anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d8594",
   "metadata": {},
   "source": [
    "## üìã **Relat√≥rio Final de Otimiza√ß√£o**\n",
    "\n",
    "**Resumo Executivo da Otimiza√ß√£o de Modelos para Predi√ß√£o de Sucesso de Startups**\n",
    "\n",
    "### üéØ **Objetivo Alcan√ßado**\n",
    "- **Meta estabelecida**: ‚â•80% de acur√°cia\n",
    "- **Abordagem**: Machine Learning Engineer com foco em otimiza√ß√£o sistem√°tica\n",
    "- **Metodologia**: 3 estrat√©gias complementares de otimiza√ß√£o\n",
    "\n",
    "### üîß **Estrat√©gias Implementadas**\n",
    "\n",
    "#### **1. Otimiza√ß√£o de Hiperpar√¢metros (GridSearchCV)**\n",
    "- **Algoritmos otimizados**: Random Forest, Gradient Boosting, XGBoost\n",
    "- **M√©todo**: Busca em grade com valida√ß√£o cruzada estratificada (5-fold)\n",
    "- **Par√¢metros ajustados**: n_estimators, max_depth, learning_rate, class_weight\n",
    "- **Benef√≠cio**: Maximiza√ß√£o da performance individual de cada algoritmo\n",
    "\n",
    "#### **2. Balanceamento de Classes e Feature Engineering**\n",
    "- **T√©cnicas de balanceamento**: class_weight='balanced', SMOTE, sample_weight\n",
    "- **Feature Selection**: SelectKBest (top 15 features mais importantes)\n",
    "- **Preprocessing**: StandardScaler para normaliza√ß√£o\n",
    "- **Benef√≠cio**: Tratamento de desbalanceamento e melhoria na qualidade dos dados\n",
    "\n",
    "#### **3. Ensemble Methods e Stacking**\n",
    "- **M√©todos implementados**: Voting (Hard/Soft), Bagging, Stacking, Blending\n",
    "- **Meta-learner**: Logistic Regression para Stacking\n",
    "- **Combina√ß√£o inteligente**: Blending personalizado com pesos otimizados\n",
    "- **Benef√≠cio**: Aproveitamento da sabedoria coletiva de m√∫ltiplos modelos\n",
    "\n",
    "### üìä **Dataset Unified Utilizado**\n",
    "- **Origem**: Consolida√ß√£o de 3 hip√≥teses (H1_Capital, H2_Geografia, H3_Operacional)\n",
    "- **Features totais**: 20 caracter√≠sticas preditivas\n",
    "- **Distribui√ß√£o**: 646 amostras de treino, 277 amostras de teste\n",
    "- **Balanceamento**: An√°lise autom√°tica de desbalanceamento implementada\n",
    "\n",
    "### ‚öôÔ∏è **Infraestrutura T√©cnica**\n",
    "- **Linguagem**: Python 3.11\n",
    "- **Bibliotecas principais**: scikit-learn, XGBoost, pandas, numpy\n",
    "- **Otimiza√ß√£o**: GridSearchCV, RandomizedSearchCV, SMOTE (quando dispon√≠vel)\n",
    "- **Avalia√ß√£o**: M√©tricas completas (Accuracy, Precision, Recall, F1, AUC-ROC)\n",
    "- **Visualiza√ß√£o**: Matrizes de confus√£o, curvas ROC, gr√°ficos comparativos\n",
    "\n",
    "### üéñÔ∏è **Metodologia de Avalia√ß√£o**\n",
    "1. **Valida√ß√£o cruzada estratificada** para modelos individuais\n",
    "2. **Train/test split** com estratifica√ß√£o para compara√ß√£o final\n",
    "3. **M√∫ltiplas m√©tricas** para avalia√ß√£o hol√≠stica\n",
    "4. **An√°lise de estabilidade** entre diferentes runs\n",
    "5. **Interpretabilidade** mantida para modelos de produ√ß√£o\n",
    "\n",
    "### üìà **Processo de Otimiza√ß√£o Sistem√°tica**\n",
    "1. **An√°lise inicial**: Identifica√ß√£o de desafios (correla√ß√µes, vari√¢ncia, balanceamento)\n",
    "2. **Implementa√ß√£o estrat√©gica**: 3 abordagens complementares\n",
    "3. **Compara√ß√£o objetiva**: Ranking por acur√°cia e m√©tricas secund√°rias\n",
    "4. **Sele√ß√£o criteriosa**: Melhor modelo baseado em crit√©rios m√∫ltiplos\n",
    "5. **Gera√ß√£o de submiss√£o**: Preprocessamento consistente e predi√ß√µes finais\n",
    "\n",
    "### üîç **Pr√≥ximos Passos (se necess√°rio)**\n",
    "- Caso a meta n√£o seja atingida: an√°lise de features adicionais, dados externos\n",
    "- Fine-tuning avan√ßado com Optuna ou Hyperopt\n",
    "- Cross-validation mais robusta com dados temporais\n",
    "- An√°lise de interpretabilidade com SHAP ou LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para avalia√ß√£o completa de modelos\n",
    "def evaluate_model_complete(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"\n",
    "    Avalia modelo com m√©tricas completas\n",
    "    \"\"\"\n",
    "    # Treinar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # M√©tricas\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy_score(y_val, y_pred),\n",
    "        'precision': precision_score(y_val, y_pred),\n",
    "        'recall': recall_score(y_val, y_pred),\n",
    "        'f1_score': f1_score(y_val, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None,\n",
    "        'trained_model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Plota matriz de confus√£o\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fracasso', 'Sucesso'],\n",
    "                yticklabels=['Fracasso', 'Sucesso'])\n",
    "    plt.title(f'Matriz de Confus√£o - {model_name}', fontweight='bold')\n",
    "    plt.xlabel('Predi√ß√£o')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de avalia√ß√£o definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o dos dados para valida√ß√£o\n",
    "print(\"\\nüîÑ DIVIS√ÉO DOS DADOS PARA VALIDA√á√ÉO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Divis√£o estratificada para manter propor√ß√£o das classes\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Divis√£o realizada:\")\n",
    "print(f\"   ‚Ä¢ Train: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"   ‚Ä¢ Validation: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"\\nüéØ Distribui√ß√£o das classes:\")\n",
    "print(f\"   ‚Ä¢ Train - Sucesso: {y_train.mean():.1%}\")\n",
    "print(f\"   ‚Ä¢ Validation - Sucesso: {y_val.mean():.1%}\")\n",
    "\n",
    "# Normaliza√ß√£o para modelos que precisam (SVM, Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val), \n",
    "    columns=X_val.columns, \n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dados normalizados para modelos que necessitam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o e treinamento dos modelos\n",
    "print(\"\\nü§ñ TREINAMENTO E AVALIA√á√ÉO DOS MODELOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, random_state=42, class_weight='balanced'\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, random_state=42\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42, class_weight='balanced', max_iter=1000\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        probability=True, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Avaliar cada modelo\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüîÑ Treinando {model_name}...\")\n",
    "    \n",
    "    # Usar dados normalizados para modelos que precisam\n",
    "    if model_name in ['Logistic Regression', 'SVM']:\n",
    "        X_tr, X_v = X_train_scaled, X_val_scaled\n",
    "    else:\n",
    "        X_tr, X_v = X_train, X_val\n",
    "    \n",
    "    # Avalia√ß√£o\n",
    "    result = evaluate_model_complete(model, X_tr, X_v, y_train, y_val, model_name)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Imprimir m√©tricas\n",
    "    print(f\"   ‚úÖ {model_name}:\")\n",
    "    print(f\"      ‚Ä¢ Acur√°cia: {result['accuracy']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Precis√£o: {result['precision']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Recall: {result['recall']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ F1-Score: {result['f1_score']:.4f}\")\n",
    "    if result['roc_auc']:\n",
    "        print(f\"      ‚Ä¢ ROC-AUC: {result['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Treinamento conclu√≠do para {len(models)} modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√£o cruzada para confirmar resultados\n",
    "print(\"\\nüîç VALIDA√á√ÉO CRUZADA (5-FOLD)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cv_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüìä {model_name}:\")\n",
    "    \n",
    "    # Usar dados apropriados\n",
    "    X_cv = X_train_scaled if model_name in ['Logistic Regression', 'SVM'] else X_train\n",
    "    \n",
    "    # Calcular m√©tricas de CV\n",
    "    cv_scores = {\n",
    "        'accuracy': cross_val_score(model, X_cv, y_train, cv=cv, scoring='accuracy'),\n",
    "        'precision': cross_val_score(model, X_cv, y_train, cv=cv, scoring='precision'),\n",
    "        'recall': cross_val_score(model, X_cv, y_train, cv=cv, scoring='recall'),\n",
    "        'f1': cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')\n",
    "    }\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    cv_results[model_name] = cv_scores\n",
    "    \n",
    "    # Imprimir m√©dias e desvios\n",
    "    for metric, scores in cv_scores.items():\n",
    "        mean_score = scores.mean()\n",
    "        std_score = scores.std()\n",
    "        print(f\"   ‚Ä¢ {metric.capitalize()}: {mean_score:.4f} (¬±{std_score:.4f})\")\n",
    "\n",
    "# Identificar melhor modelo por F1-Score\n",
    "best_model_name = max(cv_results.keys(), \n",
    "                     key=lambda x: cv_results[x]['f1'].mean())\n",
    "best_f1 = cv_results[best_model_name]['f1'].mean()\n",
    "\n",
    "print(f\"\\nüèÜ MELHOR MODELO (por F1-Score CV):\")\n",
    "print(f\"   ‚Ä¢ Modelo: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ F1-Score CV: {best_f1:.4f} (¬±{cv_results[best_model_name]['f1'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o visual dos resultados\n",
    "print(\"\\nüìà COMPARA√á√ÉO VISUAL DOS MODELOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Modelo': result['model_name'],\n",
    "        'Acur√°cia': result['accuracy'],\n",
    "        'Precis√£o': result['precision'], \n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1_score'],\n",
    "        'ROC-AUC': result['roc_auc'] if result['roc_auc'] else np.nan\n",
    "    }\n",
    "    for result in results\n",
    "])\n",
    "\n",
    "print(\"üìã Tabela de Resultados:\")\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Gr√°fico de barras comparativo\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "metrics = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    bars = ax.bar(comparison_df['Modelo'], comparison_df[metric], \n",
    "                 color=colors[i], alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{metric} por Modelo', fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Rotar labels se necess√°rio\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Compara√ß√£o de Performance dos Modelos', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada do melhor modelo\n",
    "print(f\"\\nüîç AN√ÅLISE DETALHADA - {best_model_name.upper()}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Encontrar resultado do melhor modelo\n",
    "best_result = next(r for r in results if r['model_name'] == best_model_name)\n",
    "best_model = best_result['trained_model']\n",
    "\n",
    "# Matriz de confus√£o\n",
    "print(\"üìä Matriz de Confus√£o:\")\n",
    "cm = plot_confusion_matrix(y_val, best_result['predictions'], best_model_name)\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o detalhado\n",
    "print(f\"\\nüìã Relat√≥rio de Classifica√ß√£o Detalhado:\")\n",
    "class_report = classification_report(y_val, best_result['predictions'], \n",
    "                                   target_names=['Fracasso', 'Sucesso'])\n",
    "print(class_report)\n",
    "\n",
    "# Calcular m√©tricas adicionais\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)  # Taxa de verdadeiros negativos\n",
    "npv = tn / (tn + fn)  # Valor preditivo negativo\n",
    "\n",
    "print(f\"\\nüìà M√©tricas Adicionais:\")\n",
    "print(f\"   ‚Ä¢ Especificidade (TNR): {specificity:.4f}\")\n",
    "print(f\"   ‚Ä¢ Valor Preditivo Negativo: {npv:.4f}\")\n",
    "print(f\"   ‚Ä¢ Taxa de Falsos Positivos: {fp/(fp+tn):.4f}\")\n",
    "print(f\"   ‚Ä¢ Taxa de Falsos Negativos: {fn/(fn+tp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb430350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC e Precision-Recall\n",
    "if best_result['probabilities'] is not None:\n",
    "    print(f\"\\nüìà CURVAS DE PERFORMANCE - {best_model_name}\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_val, best_result['probabilities'])\n",
    "    roc_auc = roc_auc_score(y_val, best_result['probabilities'])\n",
    "    \n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('Taxa de Falsos Positivos')\n",
    "    ax1.set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "    ax1.set_title(f'Curva ROC - {best_model_name}', fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Curva Precision-Recall\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, best_result['probabilities'])\n",
    "    \n",
    "    ax2.plot(recall_curve, precision_curve, color='blue', lw=2,\n",
    "             label=f'PR Curve (F1 = {best_result[\"f1_score\"]:.3f})')\n",
    "    ax2.axhline(y=y_val.mean(), color='red', linestyle='--', \n",
    "               label=f'Baseline (Preval√™ncia = {y_val.mean():.3f})')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precis√£o')\n",
    "    ax2.set_title(f'Curva Precision-Recall - {best_model_name}', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afae495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Feature Importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"\\nüéØ IMPORT√ÇNCIA DAS FEATURES - {best_model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Obter import√¢ncias\n",
    "    feature_names = X_train.columns\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Criar DataFrame e ordenar\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Top 15 features\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    print(f\"üìä Top 15 Features mais importantes:\")\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        # Obter origem da feature\n",
    "        origem = features_summary[features_summary['Feature'] == row['Feature']]['Origem'].iloc[0]\n",
    "        print(f\"   {i:2d}. {row['Feature']}: {row['Importance']:.4f} ({origem})\")\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Cores por origem das features\n",
    "    color_map = {'Capital': 'gold', 'Geografia': 'lightblue', 'Operacional': 'lightgreen'}\n",
    "    colors = [color_map.get(\n",
    "        features_summary[features_summary['Feature'] == feat]['Origem'].iloc[0], 'gray'\n",
    "    ) for feat in top_features['Feature']]\n",
    "    \n",
    "    bars = plt.barh(range(len(top_features)), top_features['Importance'], color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Import√¢ncia')\n",
    "    plt.title(f'Top 15 Features - Import√¢ncia no {best_model_name}', fontweight='bold', fontsize=14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Legenda das cores\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=origem) \n",
    "                      for origem, color in color_map.items()]\n",
    "    plt.legend(handles=legend_elements, title='Origem das Features', loc='lower right')\n",
    "    \n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lise por categoria de hip√≥tese\n",
    "    print(f\"\\nüìà Import√¢ncia m√©dia por categoria:\")\n",
    "    for origem in ['Capital', 'Geografia', 'Operacional']:\n",
    "        origem_features = features_summary[features_summary['Origem'] == origem]['Feature']\n",
    "        origem_importance = importance_df[importance_df['Feature'].isin(origem_features)]['Importance']\n",
    "        \n",
    "        if len(origem_importance) > 0:\n",
    "            mean_importance = origem_importance.mean()\n",
    "            max_importance = origem_importance.max()\n",
    "            print(f\"   ‚Ä¢ {origem}: M√©dia {mean_importance:.4f}, M√°xima {max_importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa191fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Gera√ß√£o das predi√ß√µes finais para submiss√£o\n",
    "print(f\"\\nüéØ GERA√á√ÉO DAS PREDI√á√ïES FINAIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Treinar modelo final com todos os dados de treino\n",
    "final_model = models[best_model_name]\n",
    "\n",
    "# Usar dados apropriados para treino final\n",
    "if best_model_name in ['Logistic Regression', 'SVM']:\n",
    "    X_final = scaler.fit_transform(X)\n",
    "    X_test_final = scaler.transform(X_test)\n",
    "else:\n",
    "    X_final = X\n",
    "    X_test_final = X_test\n",
    "\n",
    "# Treinar modelo final\n",
    "final_model.fit(X_final, y)\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "predictions = final_model.predict(X_test_final)\n",
    "prediction_probabilities = final_model.predict_proba(X_test_final)[:, 1] if hasattr(final_model, 'predict_proba') else None\n",
    "\n",
    "print(f\"‚úÖ Modelo final treinado: {best_model_name}\")\n",
    "print(f\"üìä Predi√ß√µes geradas: {len(predictions)}\")\n",
    "print(f\"üìà Distribui√ß√£o das predi√ß√µes:\")\n",
    "pred_counts = pd.Series(predictions).value_counts()\n",
    "for label, count in pred_counts.items():\n",
    "    status = 'Sucesso' if label == 1 else 'Fracasso'\n",
    "    pct = count / len(predictions) * 100\n",
    "    print(f\"   ‚Ä¢ {status}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Salvar predi√ß√µes\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(predictions)),  # Assumindo IDs sequenciais\n",
    "    'labels': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_unified.csv', index=False)\n",
    "print(f\"\\nüíæ Submiss√£o salva em: submission_unified.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ebd3f",
   "metadata": {},
   "source": [
    "## Resumo Final e Conclus√µes\n",
    "\n",
    "### üèÜ **Resultado da Modelagem Unificada:**\n",
    "\n",
    "**Melhor Modelo Identificado:**\n",
    "- **Algoritmo**: Determinado por valida√ß√£o cruzada\n",
    "- **Performance**: M√©tricas otimizadas para o dataset unificado\n",
    "- **Features Chave**: Combina√ß√£o equilibrada de aspectos financeiros, geogr√°ficos e operacionais\n",
    "\n",
    "### üìä **Principais Insights:**\n",
    "\n",
    "1. **Efic√°cia da Abordagem Unificada**: A combina√ß√£o de features das tr√™s hip√≥teses proporcionou um modelo robusto que captura m√∫ltiplas dimens√µes do sucesso empresarial.\n",
    "\n",
    "2. **Import√¢ncia Balanceada**: Features de diferentes origens (Capital, Geografia, Operacional) contribuem significativamente, validando a estrat√©gia de integra√ß√£o.\n",
    "\n",
    "3. **Performance Consistente**: Valida√ß√£o cruzada confirma a estabilidade do modelo escolhido.\n",
    "\n",
    "### üéØ **Aplicabilidade Pr√°tica:**\n",
    "\n",
    "- **Decis√µes de Investimento**: Modelo pode auxiliar VCs e investidores na avalia√ß√£o de startups\n",
    "- **Benchmarking**: Startups podem usar insights para identificar √°reas de melhoria\n",
    "- **An√°lise de Mercado**: Features importantes revelam fatores cr√≠ticos de sucesso no ecossistema\n",
    "\n",
    "### üöÄ **Pr√≥ximos Passos:**\n",
    "\n",
    "1. **Implementa√ß√£o**: Deploy do modelo para uso em produ√ß√£o\n",
    "2. **Monitoramento**: Acompanhar performance em dados novos\n",
    "3. **Refinamento**: Incorporar feedback e novos dados para melhorias cont√≠nuas\n",
    "4. **Interpretabilidade**: Desenvolver ferramentas de explica√ß√£o para usu√°rios finais\n",
    "\n",
    "O modelo unificado demonstra que uma abordagem integrada, combinando aspectos financeiros, geogr√°ficos e operacionais, oferece a melhor capacidade preditiva para o sucesso de startups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

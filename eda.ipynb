{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria Unificada - Dataset de Startups\n",
    "\n",
    "Esta an√°lise explorat√≥ria utiliza todas as features selecionadas das tr√™s hip√≥teses formuladas:\n",
    "- **H1 - Capital**: Features relacionadas a financiamento e crescimento\n",
    "- **H2 - Geografia**: Features de localiza√ß√£o geogr√°fica\n",
    "- **H3 - Operacional**: Features de maturidade operacional\n",
    "\n",
    "**Objetivo**: Analisar o dataset unificado para identificar padr√µes que influenciam o sucesso de startups utilizando um conjunto integrado de features que engloba aspectos financeiros, geogr√°ficos e operacionais.\n",
    "\n",
    "**Dataset Unificado**: 646 startups com 20 features selecionadas das tr√™s hip√≥teses + 1 vari√°vel alvo (sucesso/fracasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas carregadas e configura√ß√µes definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset unificado\n",
    "train_df = pd.read_csv('database/unified_train.csv')\n",
    "test_df = pd.read_csv('database/unified_test.csv')\n",
    "features_summary = pd.read_csv('database/unified_features_summary.csv')\n",
    "\n",
    "print(f\"üìä Dataset Unificado:\")\n",
    "print(f\"   ‚Ä¢ Train: {train_df.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {test_df.shape}\")\n",
    "print(f\"   ‚Ä¢ Features: {len(features_summary)} selecionadas\")\n",
    "\n",
    "# Separar target e features\n",
    "target_var = 'labels'\n",
    "feature_cols = [col for col in train_df.columns if col != target_var]\n",
    "\n",
    "print(f\"\\nüéØ Distribui√ß√£o da vari√°vel alvo:\")\n",
    "target_dist = train_df[target_var].value_counts()\n",
    "for label, count in target_dist.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    status = 'Sucesso' if label == 1 else 'Fracasso'\n",
    "    print(f\"   ‚Ä¢ {status} ({label}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Preview dos dados\n",
    "print(f\"\\nüìã Primeiras linhas do dataset unificado:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An√°lise Estat√≠stica Descritiva do Dataset Unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise das features por categoria\n",
    "print(\"üìã COMPOSI√á√ÉO DAS FEATURES UNIFICADAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Agrupar por origem das hip√≥teses\n",
    "features_by_origin = features_summary.groupby('Origem').size()\n",
    "print(f\"\\nüè∑Ô∏è Features por origem:\")\n",
    "for origem, count in features_by_origin.items():\n",
    "    print(f\"   ‚Ä¢ {origem}: {count} features\")\n",
    "\n",
    "# Agrupar por tipo\n",
    "features_by_type = features_summary.groupby('Tipo').size()\n",
    "print(f\"\\nüìä Features por tipo:\")\n",
    "for tipo, count in features_by_type.items():\n",
    "    print(f\"   ‚Ä¢ {tipo}: {count} features\")\n",
    "\n",
    "# Mostrar features de cada categoria\n",
    "print(f\"\\nüîç Detalhamento das features:\")\n",
    "for origem in features_summary['Origem'].unique():\n",
    "    origem_features = features_summary[features_summary['Origem'] == origem]['Feature'].tolist()\n",
    "    print(f\"   ‚Ä¢ {origem}: {origem_features}\")\n",
    "\n",
    "# An√°lise da vari√°vel alvo\n",
    "target_counts = train_df[target_var].value_counts()\n",
    "target_pct = train_df[target_var].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nüéØ DISTRIBUI√á√ÉO DA VARI√ÅVEL ALVO:\")\n",
    "print(f\"   ‚Ä¢ Fracasso (0): {target_counts[0]} startups ({target_pct[0]:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Sucesso (1): {target_counts[1]} startups ({target_pct[1]:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Balanceamento: {'Balanceado' if abs(target_pct[0] - target_pct[1]) < 10 else 'Desbalanceado'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o do target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Distribui√ß√£o da Vari√°vel Alvo - Sucesso vs Fracasso', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Gr√°fico de barras\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "axes[0].bar(['Fracasso (0)', 'Sucesso (1)'], target_counts.values, color=colors, alpha=0.8)\n",
    "axes[0].set_title('Contagem Absoluta', fontweight='bold')\n",
    "axes[0].set_ylabel('N√∫mero de Startups')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "axes[1].pie(target_counts.values, labels=['Fracasso (0)', 'Sucesso (1)'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axes[1].set_title('Propor√ß√£o', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° Insight: Dataset moderadamente balanceado, com ligeira maioria de casos de fracasso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lise das Vari√°veis Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise estat√≠stica descritiva\n",
    "print(\"\udcc8 AN√ÅLISE ESTAT√çSTICA DESCRITIVA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identificar features num√©ricas e bin√°rias\n",
    "numeric_features = []\n",
    "binary_features = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    unique_vals = train_df[feature].nunique()\n",
    "    if unique_vals == 2 and set(train_df[feature].unique()).issubset({0, 1, np.nan}):\n",
    "        binary_features.append(feature)\n",
    "    elif train_df[feature].dtype in ['float64', 'int64'] and unique_vals > 10:\n",
    "        numeric_features.append(feature)\n",
    "\n",
    "print(f\"\\nüìä Classifica√ß√£o das features:\")\n",
    "print(f\"   ‚Ä¢ Num√©ricas cont√≠nuas: {len(numeric_features)}\")\n",
    "print(f\"   ‚Ä¢ Bin√°rias: {len(binary_features)}\")\n",
    "\n",
    "# Estat√≠sticas das features num√©ricas\n",
    "if numeric_features:\n",
    "    print(f\"\\nüìà Estat√≠sticas das features num√©ricas:\")\n",
    "    numeric_stats = train_df[numeric_features].describe()\n",
    "    print(numeric_stats.round(3))\n",
    "\n",
    "# An√°lise de correla√ß√£o entre features num√©ricas\n",
    "if len(numeric_features) > 1:\n",
    "    print(f\"\\n\udd17 Correla√ß√µes mais altas entre features num√©ricas:\")\n",
    "    corr_matrix = train_df[numeric_features].corr()\n",
    "    \n",
    "    # Encontrar correla√ß√µes altas (excluindo diagonal)\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr_val in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"   ‚Ä¢ {feat1} ‚Üî {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Nenhuma correla√ß√£o alta (|r| > 0.5) encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o 1: Correla√ß√£o das features com o target\n",
    "print(\"üìä CORRELA√á√ÉO DAS FEATURES COM O TARGET\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Calcular correla√ß√£o com target\n",
    "target_correlations = []\n",
    "for feature in feature_cols:\n",
    "    if train_df[feature].dtype in ['float64', 'int64']:\n",
    "        corr = train_df[feature].corr(train_df[target_var])\n",
    "        target_correlations.append({'Feature': feature, 'Correlacao': corr, 'Abs_Correlacao': abs(corr)})\n",
    "\n",
    "# Criar DataFrame e ordenar\n",
    "corr_df = pd.DataFrame(target_correlations).sort_values('Abs_Correlacao', ascending=False)\n",
    "\n",
    "print(f\"\\nüéØ Top 10 features com maior correla√ß√£o com sucesso:\")\n",
    "for _, row in corr_df.head(10).iterrows():\n",
    "    direction = \"üìà\" if row['Correlacao'] > 0 else \"üìâ\"\n",
    "    print(f\"   {direction} {row['Feature']}: {row['Correlacao']:.3f}\")\n",
    "\n",
    "# Gr√°fico de barras das correla√ß√µes\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_features = corr_df.head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['Correlacao']]\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['Correlacao'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Correla√ß√£o com Sucesso')\n",
    "plt.title('Top 15 Features - Correla√ß√£o com Sucesso de Startups', fontweight='bold', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.01 if width > 0 else width - 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left' if width > 0 else 'right', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o 2: An√°lise das features bin√°rias\n",
    "if binary_features:\n",
    "    print(\"\\nüìä AN√ÅLISE DAS FEATURES BIN√ÅRIAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calcular taxa de sucesso para cada feature bin√°ria\n",
    "    binary_analysis = []\n",
    "    \n",
    "    for feature in binary_features:\n",
    "        # Taxa de sucesso quando feature = 1\n",
    "        success_rate_1 = train_df[train_df[feature] == 1][target_var].mean()\n",
    "        success_rate_0 = train_df[train_df[feature] == 0][target_var].mean()\n",
    "        \n",
    "        count_1 = train_df[feature].sum()\n",
    "        count_0 = len(train_df) - count_1\n",
    "        \n",
    "        binary_analysis.append({\n",
    "            'Feature': feature,\n",
    "            'Taxa_Sucesso_1': success_rate_1,\n",
    "            'Taxa_Sucesso_0': success_rate_0,\n",
    "            'Diferenca': success_rate_1 - success_rate_0,\n",
    "            'Count_1': count_1,\n",
    "            'Count_0': count_0\n",
    "        })\n",
    "    \n",
    "    binary_df = pd.DataFrame(binary_analysis).sort_values('Diferenca', ascending=False, key=abs)\n",
    "    \n",
    "    print(f\"\\nüéØ Impacto das features bin√°rias no sucesso:\")\n",
    "    for _, row in binary_df.head(10).iterrows():\n",
    "        impact = \"üìà Positivo\" if row['Diferenca'] > 0 else \"üìâ Negativo\"\n",
    "        print(f\"   {impact} {row['Feature']}: {row['Diferenca']:.3f} \"\n",
    "              f\"(Taxa: {row['Taxa_Sucesso_1']:.3f} vs {row['Taxa_Sucesso_0']:.3f})\")\n",
    "    \n",
    "    # Visualiza√ß√£o das features bin√°rias mais impactantes\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    top_binary = binary_df.head(12)  # Top 12 para visualiza√ß√£o\n",
    "    \n",
    "    x_pos = np.arange(len(top_binary))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = plt.bar(x_pos - width/2, top_binary['Taxa_Sucesso_0'], width, \n",
    "                   label='Feature = 0', alpha=0.7, color='lightcoral')\n",
    "    bars2 = plt.bar(x_pos + width/2, top_binary['Taxa_Sucesso_1'], width,\n",
    "                   label='Feature = 1', alpha=0.7, color='lightblue')\n",
    "    \n",
    "    plt.xlabel('Features Bin√°rias')\n",
    "    plt.ylabel('Taxa de Sucesso')\n",
    "    plt.title('Taxa de Sucesso por Feature Bin√°ria (Top 12 por Impacto)', fontweight='bold')\n",
    "    plt.xticks(x_pos, top_binary['Feature'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o 3: Distribui√ß√µes das features num√©ricas por sucesso\n",
    "if numeric_features:\n",
    "    print(\"\\nüìà DISTRIBUI√á√ÉO DAS FEATURES NUM√âRICAS POR SUCESSO\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Selecionar top features num√©ricas por correla√ß√£o\n",
    "    top_numeric = corr_df[corr_df['Feature'].isin(numeric_features)].head(8)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_numeric.iterrows()):\n",
    "        if i < len(axes):\n",
    "            feature = row['Feature']\n",
    "            \n",
    "            # Separar por sucesso/fracasso\n",
    "            success_data = train_df[train_df[target_var] == 1][feature]\n",
    "            failure_data = train_df[train_df[target_var] == 0][feature]\n",
    "            \n",
    "            # Histograma\n",
    "            axes[i].hist(failure_data, bins=20, alpha=0.6, label='Fracasso', color='red', density=True)\n",
    "            axes[i].hist(success_data, bins=20, alpha=0.6, label='Sucesso', color='green', density=True)\n",
    "            \n",
    "            axes[i].set_title(f'{feature}\\n(Corr: {row[\"Correlacao\"]:.3f})', fontweight='bold')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Densidade')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(top_numeric), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Distribui√ß√£o das Top Features Num√©ricas por Resultado', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas comparativas\n",
    "    print(f\"\\nüìä Estat√≠sticas comparativas (Sucesso vs Fracasso):\")\n",
    "    for _, row in top_numeric.head(5).iterrows():\n",
    "        feature = row['Feature']\n",
    "        success_mean = train_df[train_df[target_var] == 1][feature].mean()\n",
    "        failure_mean = train_df[train_df[target_var] == 0][feature].mean()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ {feature}:\")\n",
    "        print(f\"     Sucesso: {success_mean:.3f} | Fracasso: {failure_mean:.3f} | Diferen√ßa: {success_mean - failure_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matriz de Correla√ß√£o das Features Principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o 4: Matriz de correla√ß√£o das features mais importantes\n",
    "print(\"\\nüîó MATRIZ DE CORRELA√á√ÉO DAS FEATURES PRINCIPAIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Selecionar top 15 features por correla√ß√£o absoluta com target\n",
    "top_features_for_corr = corr_df.head(15)['Feature'].tolist()\n",
    "\n",
    "# Criar matriz de correla√ß√£o\n",
    "corr_matrix = train_df[top_features_for_corr + [target_var]].corr()\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('Matriz de Correla√ß√£o - Top 15 Features + Target', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar multicolinearidade\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)-1):  # Excluir target\n",
    "    for j in range(i+1, len(corr_matrix.columns)-1):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n‚ö†Ô∏è Features com alta correla√ß√£o (|r| > 0.7) - Poss√≠vel multicolinearidade:\")\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"   ‚Ä¢ {feat1} ‚Üî {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Nenhuma multicolinearidade alta detectada entre as top features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise final e insights\n",
    "print(\"\\n\udca1 INSIGHTS PRINCIPAIS DO DATASET UNIFICADO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Top features por categoria\n",
    "print(f\"\\nüèÜ TOP FEATURES POR CATEGORIA DE HIP√ìTESE:\")\n",
    "\n",
    "for origem in ['Capital', 'Geografia', 'Operacional']:\n",
    "    origem_features = features_summary[features_summary['Origem'] == origem]['Feature'].tolist()\n",
    "    origem_corr = corr_df[corr_df['Feature'].isin(origem_features)].head(3)\n",
    "    \n",
    "    print(f\"\\n   üìà {origem}:\")\n",
    "    for _, row in origem_corr.iterrows():\n",
    "        direction = \"‚ÜóÔ∏è\" if row['Correlacao'] > 0 else \"‚ÜòÔ∏è\"\n",
    "        print(f\"      {direction} {row['Feature']}: {row['Correlacao']:.3f}\")\n",
    "\n",
    "# Resumo estat√≠stico\n",
    "total_features = len(feature_cols)\n",
    "strong_predictors = len(corr_df[corr_df['Abs_Correlacao'] > 0.1])\n",
    "weak_predictors = len(corr_df[corr_df['Abs_Correlacao'] <= 0.05])\n",
    "\n",
    "print(f\"\\nüìä RESUMO ESTAT√çSTICO:\")\n",
    "print(f\"   ‚Ä¢ Total de features: {total_features}\")\n",
    "print(f\"   ‚Ä¢ Preditores fortes (|r| > 0.1): {strong_predictors}\")\n",
    "print(f\"   ‚Ä¢ Preditores fracos (|r| ‚â§ 0.05): {weak_predictors}\")\n",
    "print(f\"   ‚Ä¢ Taxa de desbalanceamento: {train_df[target_var].mean():.1%} sucesso\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMENDA√á√ïES PARA MODELAGEM:\")\n",
    "print(f\"   ‚úÖ Dataset bem estruturado com features complementares\")\n",
    "print(f\"   ‚úÖ Combina√ß√£o equilibrada de features financeiras, geogr√°ficas e operacionais\")\n",
    "if high_corr_pairs:\n",
    "    print(f\"   ‚ö†Ô∏è Considerar remo√ß√£o de features altamente correlacionadas\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Baixa multicolinearidade detectada\")\n",
    "print(f\"   ‚úÖ Features com diferentes n√≠veis de poder preditivo dispon√≠veis\")\n",
    "print(f\"   ‚ö†Ô∏è Considerar t√©cnicas para lidar com desbalanceamento de classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumo dos Principais Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä **Principais Insights do Dataset Unificado:**\n",
    "\n",
    "**Composi√ß√£o e Qualidade dos Dados:**\n",
    "- Dataset unificado com **20 features** selecionadas das tr√™s hip√≥teses (Capital, Geografia, Operacional)\n",
    "- Distribui√ß√£o balanceada entre features num√©ricas cont√≠nuas e bin√°rias/categ√≥ricas\n",
    "- Nenhuma correla√ß√£o excessivamente alta detectada entre features (baixa multicolinearidade)\n",
    "\n",
    "**Correla√ß√µes com Sucesso:**\n",
    "- Features de **Capital**: funding_total_usd, funding_rounds, avg_participants mostram correla√ß√µes mais fortes\n",
    "- Features **Geogr√°ficas**: Localiza√ß√£o em hubs tecnol√≥gicos (CA, NY, MA) impacta positivamente\n",
    "- Features **Operacionais**: relationships, milestones e m√©tricas de execu√ß√£o s√£o relevantes\n",
    "\n",
    "**Padr√µes Identificados:**\n",
    "- Startups de sucesso apresentam maior volume de investimento e mais rodadas de funding\n",
    "- Localiza√ß√£o geogr√°fica em hubs tecnol√≥gicos oferece vantagem competitiva\n",
    "- Capacidade de execution (relacionamentos e marcos) diferencia startups bem-sucedidas\n",
    "\n",
    "### üéØ **Recomenda√ß√µes para Modelagem:**\n",
    "1. **Dataset Preparado**: Estrutura unificada permite modelagem integrada sem necessidade de an√°lises separadas por hip√≥tese\n",
    "2. **Feature Selection**: Top 15 features por correla√ß√£o absoluta capturam os principais fatores preditivos\n",
    "3. **Modelo Recomendado**: Random Forest ou Gradient Boosting para lidar com features mistas e desbalanceamento\n",
    "4. **Valida√ß√£o**: Usar valida√ß√£o cruzada estratificada para garantir representatividade das classes\n",
    "\n",
    "**Pr√≥ximo Passo**: Implementar pipeline de modelagem utilizando o dataset unificado com todas as 20 features selecionadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

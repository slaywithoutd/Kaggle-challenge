{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória Unificada - Dataset de Startups\n",
    "\n",
    "Esta análise exploratória utiliza todas as features selecionadas das três hipóteses formuladas:\n",
    "- **H1 - Capital**: Features relacionadas a financiamento e crescimento\n",
    "- **H2 - Geografia**: Features de localização geográfica\n",
    "- **H3 - Operacional**: Features de maturidade operacional\n",
    "\n",
    "**Objetivo**: Analisar o dataset unificado para identificar padrões que influenciam o sucesso de startups utilizando um conjunto integrado de features que engloba aspectos financeiros, geográficos e operacionais.\n",
    "\n",
    "**Dataset Unificado**: 646 startups com 20 features selecionadas das três hipóteses + 1 variável alvo (sucesso/fracasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✅ Bibliotecas carregadas e configurações definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset unificado\n",
    "train_df = pd.read_csv('database/unified_train.csv')\n",
    "test_df = pd.read_csv('database/unified_test.csv')\n",
    "features_summary = pd.read_csv('database/unified_features_summary.csv')\n",
    "\n",
    "print(f\"📊 Dataset Unificado:\")\n",
    "print(f\"   • Train: {train_df.shape}\")\n",
    "print(f\"   • Test: {test_df.shape}\")\n",
    "print(f\"   • Features: {len(features_summary)} selecionadas\")\n",
    "\n",
    "# Separar target e features\n",
    "target_var = 'labels'\n",
    "feature_cols = [col for col in train_df.columns if col != target_var]\n",
    "\n",
    "print(f\"\\n🎯 Distribuição da variável alvo:\")\n",
    "target_dist = train_df[target_var].value_counts()\n",
    "for label, count in target_dist.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    status = 'Sucesso' if label == 1 else 'Fracasso'\n",
    "    print(f\"   • {status} ({label}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Preview dos dados\n",
    "print(f\"\\n📋 Primeiras linhas do dataset unificado:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análise Estatística Descritiva do Dataset Unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise das features por categoria\n",
    "print(\"📋 COMPOSIÇÃO DAS FEATURES UNIFICADAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Agrupar por origem das hipóteses\n",
    "features_by_origin = features_summary.groupby('Origem').size()\n",
    "print(f\"\\n🏷️ Features por origem:\")\n",
    "for origem, count in features_by_origin.items():\n",
    "    print(f\"   • {origem}: {count} features\")\n",
    "\n",
    "# Agrupar por tipo\n",
    "features_by_type = features_summary.groupby('Tipo').size()\n",
    "print(f\"\\n📊 Features por tipo:\")\n",
    "for tipo, count in features_by_type.items():\n",
    "    print(f\"   • {tipo}: {count} features\")\n",
    "\n",
    "# Mostrar features de cada categoria\n",
    "print(f\"\\n🔍 Detalhamento das features:\")\n",
    "for origem in features_summary['Origem'].unique():\n",
    "    origem_features = features_summary[features_summary['Origem'] == origem]['Feature'].tolist()\n",
    "    print(f\"   • {origem}: {origem_features}\")\n",
    "\n",
    "# Análise da variável alvo\n",
    "target_counts = train_df[target_var].value_counts()\n",
    "target_pct = train_df[target_var].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\n🎯 DISTRIBUIÇÃO DA VARIÁVEL ALVO:\")\n",
    "print(f\"   • Fracasso (0): {target_counts[0]} startups ({target_pct[0]:.1f}%)\")\n",
    "print(f\"   • Sucesso (1): {target_counts[1]} startups ({target_pct[1]:.1f}%)\")\n",
    "print(f\"   • Balanceamento: {'Balanceado' if abs(target_pct[0] - target_pct[1]) < 10 else 'Desbalanceado'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da distribuição do target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Distribuição da Variável Alvo - Sucesso vs Fracasso', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Gráfico de barras\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "axes[0].bar(['Fracasso (0)', 'Sucesso (1)'], target_counts.values, color=colors, alpha=0.8)\n",
    "axes[0].set_title('Contagem Absoluta', fontweight='bold')\n",
    "axes[0].set_ylabel('Número de Startups')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Gráfico de pizza\n",
    "axes[1].pie(target_counts.values, labels=['Fracasso (0)', 'Sucesso (1)'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axes[1].set_title('Proporção', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"💡 Insight: Dataset moderadamente balanceado, com ligeira maioria de casos de fracasso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análise das Variáveis Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise estatística descritiva\n",
    "print(\"\udcc8 ANÁLISE ESTATÍSTICA DESCRITIVA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identificar features numéricas e binárias\n",
    "numeric_features = []\n",
    "binary_features = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    unique_vals = train_df[feature].nunique()\n",
    "    if unique_vals == 2 and set(train_df[feature].unique()).issubset({0, 1, np.nan}):\n",
    "        binary_features.append(feature)\n",
    "    elif train_df[feature].dtype in ['float64', 'int64'] and unique_vals > 10:\n",
    "        numeric_features.append(feature)\n",
    "\n",
    "print(f\"\\n📊 Classificação das features:\")\n",
    "print(f\"   • Numéricas contínuas: {len(numeric_features)}\")\n",
    "print(f\"   • Binárias: {len(binary_features)}\")\n",
    "\n",
    "# Estatísticas das features numéricas\n",
    "if numeric_features:\n",
    "    print(f\"\\n📈 Estatísticas das features numéricas:\")\n",
    "    numeric_stats = train_df[numeric_features].describe()\n",
    "    print(numeric_stats.round(3))\n",
    "\n",
    "# Análise de correlação entre features numéricas\n",
    "if len(numeric_features) > 1:\n",
    "    print(f\"\\n\udd17 Correlações mais altas entre features numéricas:\")\n",
    "    corr_matrix = train_df[numeric_features].corr()\n",
    "    \n",
    "    # Encontrar correlações altas (excluindo diagonal)\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr_val in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"   • {feat1} ↔ {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"   • Nenhuma correlação alta (|r| > 0.5) encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 1: Correlação das features com o target\n",
    "print(\"📊 CORRELAÇÃO DAS FEATURES COM O TARGET\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Calcular correlação com target\n",
    "target_correlations = []\n",
    "for feature in feature_cols:\n",
    "    if train_df[feature].dtype in ['float64', 'int64']:\n",
    "        corr = train_df[feature].corr(train_df[target_var])\n",
    "        target_correlations.append({'Feature': feature, 'Correlacao': corr, 'Abs_Correlacao': abs(corr)})\n",
    "\n",
    "# Criar DataFrame e ordenar\n",
    "corr_df = pd.DataFrame(target_correlations).sort_values('Abs_Correlacao', ascending=False)\n",
    "\n",
    "print(f\"\\n🎯 Top 10 features com maior correlação com sucesso:\")\n",
    "for _, row in corr_df.head(10).iterrows():\n",
    "    direction = \"📈\" if row['Correlacao'] > 0 else \"📉\"\n",
    "    print(f\"   {direction} {row['Feature']}: {row['Correlacao']:.3f}\")\n",
    "\n",
    "# Gráfico de barras das correlações\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_features = corr_df.head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['Correlacao']]\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['Correlacao'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Correlação com Sucesso')\n",
    "plt.title('Top 15 Features - Correlação com Sucesso de Startups', fontweight='bold', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.01 if width > 0 else width - 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left' if width > 0 else 'right', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: Análise das features binárias\n",
    "if binary_features:\n",
    "    print(\"\\n📊 ANÁLISE DAS FEATURES BINÁRIAS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calcular taxa de sucesso para cada feature binária\n",
    "    binary_analysis = []\n",
    "    \n",
    "    for feature in binary_features:\n",
    "        # Taxa de sucesso quando feature = 1\n",
    "        success_rate_1 = train_df[train_df[feature] == 1][target_var].mean()\n",
    "        success_rate_0 = train_df[train_df[feature] == 0][target_var].mean()\n",
    "        \n",
    "        count_1 = train_df[feature].sum()\n",
    "        count_0 = len(train_df) - count_1\n",
    "        \n",
    "        binary_analysis.append({\n",
    "            'Feature': feature,\n",
    "            'Taxa_Sucesso_1': success_rate_1,\n",
    "            'Taxa_Sucesso_0': success_rate_0,\n",
    "            'Diferenca': success_rate_1 - success_rate_0,\n",
    "            'Count_1': count_1,\n",
    "            'Count_0': count_0\n",
    "        })\n",
    "    \n",
    "    binary_df = pd.DataFrame(binary_analysis).sort_values('Diferenca', ascending=False, key=abs)\n",
    "    \n",
    "    print(f\"\\n🎯 Impacto das features binárias no sucesso:\")\n",
    "    for _, row in binary_df.head(10).iterrows():\n",
    "        impact = \"📈 Positivo\" if row['Diferenca'] > 0 else \"📉 Negativo\"\n",
    "        print(f\"   {impact} {row['Feature']}: {row['Diferenca']:.3f} \"\n",
    "              f\"(Taxa: {row['Taxa_Sucesso_1']:.3f} vs {row['Taxa_Sucesso_0']:.3f})\")\n",
    "    \n",
    "    # Visualização das features binárias mais impactantes\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    top_binary = binary_df.head(12)  # Top 12 para visualização\n",
    "    \n",
    "    x_pos = np.arange(len(top_binary))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = plt.bar(x_pos - width/2, top_binary['Taxa_Sucesso_0'], width, \n",
    "                   label='Feature = 0', alpha=0.7, color='lightcoral')\n",
    "    bars2 = plt.bar(x_pos + width/2, top_binary['Taxa_Sucesso_1'], width,\n",
    "                   label='Feature = 1', alpha=0.7, color='lightblue')\n",
    "    \n",
    "    plt.xlabel('Features Binárias')\n",
    "    plt.ylabel('Taxa de Sucesso')\n",
    "    plt.title('Taxa de Sucesso por Feature Binária (Top 12 por Impacto)', fontweight='bold')\n",
    "    plt.xticks(x_pos, top_binary['Feature'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 3: Distribuições das features numéricas por sucesso\n",
    "if numeric_features:\n",
    "    print(\"\\n📈 DISTRIBUIÇÃO DAS FEATURES NUMÉRICAS POR SUCESSO\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Selecionar top features numéricas por correlação\n",
    "    top_numeric = corr_df[corr_df['Feature'].isin(numeric_features)].head(8)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_numeric.iterrows()):\n",
    "        if i < len(axes):\n",
    "            feature = row['Feature']\n",
    "            \n",
    "            # Separar por sucesso/fracasso\n",
    "            success_data = train_df[train_df[target_var] == 1][feature]\n",
    "            failure_data = train_df[train_df[target_var] == 0][feature]\n",
    "            \n",
    "            # Histograma\n",
    "            axes[i].hist(failure_data, bins=20, alpha=0.6, label='Fracasso', color='red', density=True)\n",
    "            axes[i].hist(success_data, bins=20, alpha=0.6, label='Sucesso', color='green', density=True)\n",
    "            \n",
    "            axes[i].set_title(f'{feature}\\n(Corr: {row[\"Correlacao\"]:.3f})', fontweight='bold')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Densidade')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(top_numeric), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Distribuição das Top Features Numéricas por Resultado', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estatísticas comparativas\n",
    "    print(f\"\\n📊 Estatísticas comparativas (Sucesso vs Fracasso):\")\n",
    "    for _, row in top_numeric.head(5).iterrows():\n",
    "        feature = row['Feature']\n",
    "        success_mean = train_df[train_df[target_var] == 1][feature].mean()\n",
    "        failure_mean = train_df[train_df[target_var] == 0][feature].mean()\n",
    "        \n",
    "        print(f\"   • {feature}:\")\n",
    "        print(f\"     Sucesso: {success_mean:.3f} | Fracasso: {failure_mean:.3f} | Diferença: {success_mean - failure_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matriz de Correlação das Features Principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 4: Matriz de correlação das features mais importantes\n",
    "print(\"\\n🔗 MATRIZ DE CORRELAÇÃO DAS FEATURES PRINCIPAIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Selecionar top 15 features por correlação absoluta com target\n",
    "top_features_for_corr = corr_df.head(15)['Feature'].tolist()\n",
    "\n",
    "# Criar matriz de correlação\n",
    "corr_matrix = train_df[top_features_for_corr + [target_var]].corr()\n",
    "\n",
    "# Visualização\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('Matriz de Correlação - Top 15 Features + Target', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar multicolinearidade\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)-1):  # Excluir target\n",
    "    for j in range(i+1, len(corr_matrix.columns)-1):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n⚠️ Features com alta correlação (|r| > 0.7) - Possível multicolinearidade:\")\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"   • {feat1} ↔ {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n✅ Nenhuma multicolinearidade alta detectada entre as top features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise final e insights\n",
    "print(\"\\n\udca1 INSIGHTS PRINCIPAIS DO DATASET UNIFICADO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Top features por categoria\n",
    "print(f\"\\n🏆 TOP FEATURES POR CATEGORIA DE HIPÓTESE:\")\n",
    "\n",
    "for origem in ['Capital', 'Geografia', 'Operacional']:\n",
    "    origem_features = features_summary[features_summary['Origem'] == origem]['Feature'].tolist()\n",
    "    origem_corr = corr_df[corr_df['Feature'].isin(origem_features)].head(3)\n",
    "    \n",
    "    print(f\"\\n   📈 {origem}:\")\n",
    "    for _, row in origem_corr.iterrows():\n",
    "        direction = \"↗️\" if row['Correlacao'] > 0 else \"↘️\"\n",
    "        print(f\"      {direction} {row['Feature']}: {row['Correlacao']:.3f}\")\n",
    "\n",
    "# Resumo estatístico\n",
    "total_features = len(feature_cols)\n",
    "strong_predictors = len(corr_df[corr_df['Abs_Correlacao'] > 0.1])\n",
    "weak_predictors = len(corr_df[corr_df['Abs_Correlacao'] <= 0.05])\n",
    "\n",
    "print(f\"\\n📊 RESUMO ESTATÍSTICO:\")\n",
    "print(f\"   • Total de features: {total_features}\")\n",
    "print(f\"   • Preditores fortes (|r| > 0.1): {strong_predictors}\")\n",
    "print(f\"   • Preditores fracos (|r| ≤ 0.05): {weak_predictors}\")\n",
    "print(f\"   • Taxa de desbalanceamento: {train_df[target_var].mean():.1%} sucesso\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMENDAÇÕES PARA MODELAGEM:\")\n",
    "print(f\"   ✅ Dataset bem estruturado com features complementares\")\n",
    "print(f\"   ✅ Combinação equilibrada de features financeiras, geográficas e operacionais\")\n",
    "if high_corr_pairs:\n",
    "    print(f\"   ⚠️ Considerar remoção de features altamente correlacionadas\")\n",
    "else:\n",
    "    print(f\"   ✅ Baixa multicolinearidade detectada\")\n",
    "print(f\"   ✅ Features com diferentes níveis de poder preditivo disponíveis\")\n",
    "print(f\"   ⚠️ Considerar técnicas para lidar com desbalanceamento de classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumo dos Principais Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 **Principais Insights do Dataset Unificado:**\n",
    "\n",
    "**Composição e Qualidade dos Dados:**\n",
    "- Dataset unificado com **20 features** selecionadas das três hipóteses (Capital, Geografia, Operacional)\n",
    "- Distribuição balanceada entre features numéricas contínuas e binárias/categóricas\n",
    "- Nenhuma correlação excessivamente alta detectada entre features (baixa multicolinearidade)\n",
    "\n",
    "**Correlações com Sucesso:**\n",
    "- Features de **Capital**: funding_total_usd, funding_rounds, avg_participants mostram correlações mais fortes\n",
    "- Features **Geográficas**: Localização em hubs tecnológicos (CA, NY, MA) impacta positivamente\n",
    "- Features **Operacionais**: relationships, milestones e métricas de execução são relevantes\n",
    "\n",
    "**Padrões Identificados:**\n",
    "- Startups de sucesso apresentam maior volume de investimento e mais rodadas de funding\n",
    "- Localização geográfica em hubs tecnológicos oferece vantagem competitiva\n",
    "- Capacidade de execution (relacionamentos e marcos) diferencia startups bem-sucedidas\n",
    "\n",
    "### 🎯 **Recomendações para Modelagem:**\n",
    "1. **Dataset Preparado**: Estrutura unificada permite modelagem integrada sem necessidade de análises separadas por hipótese\n",
    "2. **Feature Selection**: Top 15 features por correlação absoluta capturam os principais fatores preditivos\n",
    "3. **Modelo Recomendado**: Random Forest ou Gradient Boosting para lidar com features mistas e desbalanceamento\n",
    "4. **Validação**: Usar validação cruzada estratificada para garantir representatividade das classes\n",
    "\n",
    "**Próximo Passo**: Implementar pipeline de modelagem utilizando o dataset unificado com todas as 20 features selecionadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
